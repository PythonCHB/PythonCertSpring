\documentclass{beamer}
%\usepackage[latin1]{inputenc}
\usetheme{Warsaw}
\title[Python Certificate: System Development]{System Development with Python: Week 8}
\author{Christopher Barker}
\institute{UW Continuing Education}
\date{May 14, 2013}

\usepackage{listings}
\usepackage{hyperref}

\begin{document}

% ---------------------------------------------
\begin{frame}
  \titlepage
\end{frame}

% ---------------------------------------------
\begin{frame}
\frametitle{Table of Contents}
%\tableofcontents[currentsection]
  \tableofcontents
\end{frame}


\section{Introduction}

% ---------------------------------------------
\begin{frame}[fragile]{Performance Testing}

{\Large ``Premature optimization is the root of all evil''}\\[0.1in]
{\large \hspace{0.5in} -- Donald Knuth}

\end{frame} 

\begin{frame}[fragile]{Optimizing}

{\Large Optimizing procedure:}

\begin{enumerate}
  \item Get it right.
  \item Test it's right.
  \item Profile if slow.
  \item Optimise (the bits that matter)
  \item Repeat from 2 (until it's good enough)
\end{enumerate}

\end{frame}

\begin{frame}[fragile]{Optimizing}

{\Large Anecdote:}


\vfill
{\large I once optimized a portion of the start-up routine for a oil spill modeling project -- numpy+custom C extensions}

\vfill
{\large The portion I worked on I sped up by a factor of 100}

\vfill
{\large That made the start-up time twice as fast}

\vfill
{\large But the start up took 20 seconds (was 40), and the whole run took hours.}

\vfill
{\large It was a total waste of my time (though I learned a lot)}

\end{frame}


% ---------------------------------------------
\begin{frame}[fragile]{Profiling/timing}

\vfill
{\Large You can't optimize your code without knowing where the bottlenecks are.}

\vfill
{\Large Smarter people than me have said they they are almost always wrong
when they try to logically determine where the slow code is. (I know I am)}

\vfill
{\Large ... and how to speed it up}

\vfill
{\Large Profiling: finding where the bottlenecks are}

\vfill
{\Large Timing: Comparing different implementations}

\end{frame} 


\section{Timing}

% ---------------------------------------------
\begin{frame}[fragile]{time.clock()}

{\Large The really easy way:}

\begin{verbatim}
import time

start = time.clock()
  ... do_some_stuff ...
print "It took %f seconds to run"%(time.clock - start)
\end{verbatim}

{\Large It works, it's easy, and it gives a gross approximation}

\vfill
(use \verb|time.clock()|, rather than \verb|time.time()|, unless you want to time network access, etc.)

\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{timeit}

{\Large The good way:}

\begin{verbatim}
import timeit

timeit.timeit( statement, setup=some_stuff)

\end{verbatim}

{\Large It's kind of a pain, but gives meaningful results.}

(can also be called on the command line)

\vfill
\url{http://docs.python.org/library/timeit.html}
\vfill
(\verb|code/timing.py|)
\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{\%timeit}

{\Large The easy {\bf and} good way:}

\vfill
{\LARGE \verb|iPython|:}
\begin{verbatim}

In [52]: import timing

In [53]: %timeit timing.primes_stupid(5)
100000 loops, best of 3: 10.9 us per loop

\end{verbatim}

{\Large Takes care of the setup/namespace and number of iterations for you}

\vfill
(demo)

\vfill
\url{http://ipython.org/ipython-doc/dev/interactive/tutorial.html}
\end{frame} 


% ---------------------------------------------
\begin{frame}[fragile]{timing}

{\LARGE NOTE:}

\vfill
{\Large All these timing methods depend a lot on hardware, system load, etc.}


\vfill
{\Large So really only good for relative timing -- comparing one method to another -- on the same machine, under same load.}

\end{frame}

%-------------------------------
\begin{frame}[fragile]{LAB}

{\Large Timing Lab}
\begin{itemize}
  \item run \verb|timeit| on some code of yours (or timing.py, or..)
  \item run iPython's \%timeit on the same code.
  \item try to make the primes code in timing.py faster, and time the difference.
  \item write some code that tests one of the performance issues in:\\
        {\small \url{http://wiki.python.org/moin/PythonSpeed/PerformanceTips} }\\
        use one of the \verb|timeit|s to see if you can make a difference.
  \item Is string concatenation really slower than building in a list and joining?
\end{itemize}

\end{frame}


\section{Profiling}

% ---------------------------------------------
\begin{frame}[fragile]{Profiling}

{\Large A profiler is a tool that describes the run time performance of a
program, providing a variety of statistics}

\vfill
{\Large Helpful when you don't yet know where your bottlenecks are}

\vfill
{\Large The python profiler}

\begin{verbatim}
python -m cProfile -o profile_dump profile_example.py  
\end{verbatim}
{\Large spews some stats}


\vfill
\url{http://docs.python.org/library/profile.html}
\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{python profiler}

{\Large What you get:}

\begin{description}
  \item[ncalls] the number of calls.
  \item[tottime] the total time spent in the given function (and excluding time made in calls to sub-functions),
  \item[percall] the quotient of tottime divided by ncalls
  \item[cumtime] the total time spent in this and all subfunctions (from invocation till exit). This figure is accurate even for recursive functions.
  \item[percall] the quotient of cumtime divided by primitive calls
\end{description}
(demo: \verb|python -m cProfile profile_example.py|)
\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{python profiler}

{\Large You can also dump to a file:}

\vfill
{\verb|$ python -m cProfile -o profile_dump profile_example.py|}

\vfill
{\large This gives you a binary file you can examine with \verb|pstats|:}

\vfill
{demo: \verb|$ python -m pstats|}

\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{pstats}

{\Large Running \verb|pstats|}
{\small
\begin{verbatim}
$
$ python -m pstats
Welcome to the profile statistics browser.
% read profile_dump
profile_dump% stats
Wed Aug 29 16:21:39 2012    profile_dump

         51403 function calls in 0.032 seconds

   Random listing order was used

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    51200    0.006    0.000    0.006    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.032    0.032 profile_example.py:9(<module>)
        1    0.001    0.001    0.032    0.032 profile_example.py:28(main)
      100    0.022    0.000    0.027    0.000 profile_example.py:11(odd_numbers)
      100    0.003    0.000    0.031    0.000 profile_example.py:19(sum_odd_numbers)
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
profile_dump% 
\end{verbatim}
}
\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{pstats commands}

{\Large Commands:}

\begin{description}
  \item[help] help on pstats or particular command
  \item[stats] print the profile statistics
  \item[sort] sort by various data fields
  \item[strip] strips the leading path info from file names
  \item[callers] Print callers statistics
  \item[callees] Print callees statistics
  \item[quit] quits
\end{description}
{\large Each has options to customize output}

\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{automating profile stats}

{\Large \verb|cProfile| and \verb|pstats| are also modules}

\vfill
{\Large So you can script collection of profiles and stats}

\vfill
\url{http://docs.python.org/library/profile.html}

\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{``Run Snake Run''}

\vfill
{\Large For a visual look at your profiling results:}

\vfill
\url{http://www.vrplumber.com/programming/runsnakerun/}

\vfill
(pretty cool stuff!)

\end{frame} 

%-------------------------------
\begin{frame}[fragile]{line profiler}

\vfill
{\large \verb|$ pip install line_profile|}

\vfill
{\large decorate the function you want to profile:}

\begin{verbatim}
@profile
def primes_stupid(N):
    ...
\end{verbatim}

\vfill
{\large decorate the function you want to profile:}

{\large \verb|$ kernprof.py -l -v line_prof_example.py|}


\vfill
\url{http://pythonhosted.org/line_profiler/}

\vfill
\url{http://www.huyng.com/posts/python-performance-analysis/}

\end{frame}


\section{Performance Tuning}

% ---------------------------------------------

\begin{frame}[fragile]{``big O'' notation}

\vfill
{\Large Computer scientists describe the efficency of an algorithm in ``big O'' notation.}

\vfill
{\Large It describes how much longer a algorithm takes as a function of how much data it is working with}

\vfill
\begin{description}
  \item[O(1)] time stays the same regardless of how much data (adding to dicts)
  \item[O(n)] goes up linearly with number of items\\
              (searching lists)
  \item[$O(n^2)$] goes up quadratically with number of items\\
  \item[$O(log(n))$] goes up with the log of how many items\\
                     (bisection search)
\end{description}

Choosing the right data structure / algorithm is key.

\url{http://wiki.python.org/moin/TimeComplexity}

\end{frame} 


% ---------------------------------------------
\begin{frame}[fragile]{Performance Tips}

\vfill
{\Large Some common python performance issues:}

\begin{itemize}
  \item function calls can be slow(ish) -- sometimes worth in-lining.
  \item looking up names -- particular global ones:\\
        can be worth making local copies
  \item looping: list comps, maps, etc {\bf can} be faster.
\end{itemize}

\vfill
\url{http://wiki.python.org/moin/PythonSpeed/PerformanceTips/}

\vfill
(some nifty profiling tools described there, too)

\end{frame} 

% ---------------------------------------------
\begin{frame}[fragile]{Memory}

\vfill
{\LARGE Don't forget memory:}

\vfill
{\Large Processors are fast}

\vfill
{\Large It often takes longer to push the memory around than do the computation}

\vfill
{\Large So keep in in mind for big data sets:}
\begin{itemize}
   \item Use efficient data structures: \verb|array.array|, numpy
   \item Use efficient algorithms ( $O( \log n )$ )
   \item Use generators, rather than lists: \verb|xrange|, etc.
   \item Use suck in the data you need from databases, etc.  
\end{itemize}


\vfill
\url{http://wiki.python.org/moin/PythonSpeed/PerformanceTips/}

and:

\url{http://www.python.org/doc/essays/list2str.html}

\end{frame} 

%-------------------------------
\begin{frame}[fragile]{for loops redux...}

{\LARGE Are for loops really slow?}

\vfill
{\Large From Linkedin Discussion:}

\vfill
{\large For loop is extremely slow}

I retrieved 2 million lines of data from the database (each line has two columns), and i would like to restructure the tuple-format data to a dict,

it's really unacceptable in case i use a for loop, it takes a lot of memory and very very slow.

finally after long time searching for the solution in the network, i replace the for loop with a map + lambda. i think it's hundreds of times faster.


\vfill
demo: \verb|for_loop_test.ipynb|

\end{frame} 

%-------------------------------
\begin{frame}[fragile]{LAB}

{\Large Profiling lab}
\begin{itemize}
  \item try the profile tutorial at:\\
        {\small \url{http://pysnippet.blogspot.com/2009/12/profiling-your-python-code.html} }

  \item run cprofile on your code:\\
        any surprises? 
\end{itemize}

\end{frame}


%-------------------------------
\begin{frame}[fragile]{Wrap up}

\vfill
{\Large I hope you have an idea how to profile and time your code.}
\vfill
{\Large Try it on a part of your project}
\vfill

\end{frame}

%-------------------------------
\begin{frame}{Next Week:}

\vfill
{\LARGE wxPython Desktop GUIs}

\vfill

\end{frame}

%-------------------------------
\begin{frame}[fragile]{Project Time!}

\vfill
\Large{Profile your project}

\vfill
\Large{Performance tune part of it}

\vfill
\Large{Get ready to present}

\vfill
\Large{Anyone want a public code review?}

\end{frame}

\end{document}

 
